# ContextShift ğŸ§ ğŸ’¬  
*A prototype for user-controlled memory trimming in LLM-based chat interfaces*

## ğŸ” Overview

**ContextShift** is a Streamlit-based prototype demonstrating a key feature proposed for future LLM chat interfaces:

> **Manual context management** â€“ Let users delete older chat messages to free up space within a fixed token window, instead of forcing them to start a new conversation.

This feature mimics how long-context autoregressive LLMs (like ChatGPT or Claude) struggle with efficiency and cost as history grows. ContextShift gives users lightweight control over this by letting them manually trim the conversation.

---

## ğŸš€ Features

- âœ… Simulated LLM chat interface with user and AI messages  
- ğŸš« Message cap at 10 exchanges to simulate token window limit  
- âš ï¸ Warning when context is full  
- ğŸ—‘ï¸ Delete button for any previous message to free space  
- ğŸ”„ Dynamic UI update after deletion  
- ğŸ“œ Scrollable message view for better UX  

---

## ğŸ› ï¸ How It Works

- Each user+AI message pair counts as 1 message
  
- After 10 messages, input is disabled until the user deletes one or more

- Deleting messages drops the message count and re-enables input
  
- This simulates context trimming without retraining the model or restarting the session

---

## ğŸ“‚ File Structure

ğŸ“¦contextshift

    â”£ ğŸ“„ app.py              # Main Streamlit app (your script)
    
    â”£ ğŸ“„ README.md


---

---

## ğŸ§ª Demo Logic

The system is built on top of `st.session_state` to simulate persistent memory across interactions.

python

    if len(st.session_state.messages) >= 10:

    st.session_state.chat_full = True 

Users can click a Delete button next to each message to:

 Trim the message history manually
 
 Free up the token budget (simulated via a message limit)
 
 Keep the chat thread alive without starting over



ğŸ“¦ Setup Instructions

1. Clone this repo
   
       git clone https://github.com/Vijay6923/ContextShift.git
       
       cd contextshift
   
2. Create a virtual environment (optional but recommended)
   
       python -m venv venv
   
       source venv/bin/activate  # or venv\Scripts\activate on Windows
   
3. Install requirements

       pip install streamlit

4. Run the app

       streamlit run app.py


ğŸ¯ Why This Matters

In real LLM systems, long-term chats hit token limits, leading to:

 Lost context

 Slower response time
 
 Higher inference costs
 
 Frustrated users

ContextShift is a step toward user-facing memory management â€” where power users can manually prune irrelevant past interactions, improving efficiency without architectural changes.

This is part of a broader proposal to make LLM interfaces:

   More transparent
  
   More efficient
  
   More usable for real work (coding, research, tutoring, etc.)

ğŸ§  Next Steps (Ideas to Extend)

   Track actual token count per message using tiktoken
   
   Add â€œCollapseâ€ instead of â€œDeleteâ€ (to simulate summarization)
  
   Add AI-suggested trimming (model recommends irrelevant messages to delete)
  
   Add save/bookmark for important messages

ğŸ¤ Contributing & Discussion

This is part of a larger set of UX ideas Iâ€™m building for LLM products.

If youâ€™re interested in collaborating or brainstorming more:

  ğŸ“§ Email: vk29082023@gmail.com

